<!DOCTYPE html>
<html lang="en">
<head>
<!-- 2026-02-17 Tue 14:28 -->
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Adversarial search</title>
<meta name="author" content="user0" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="/css/tufte.css" />
<link rel="stylesheet" type="text/css" href="/css/ox-tufte.css" />
</head>
<body>
<article id="content" class="content">
<header>
<h1 class="title">Adversarial search</h1>
</header><p>
Games: different types of games:
deterministic vs stochastic, multi player vs single player , zero sum ,fully observable vs partially observable
</p>

<p>
deterministic games
States, players, actions, transition functions, terminal test, terminal utilities
</p>

<p>
Solution is policy (s -&gt; a)
</p>
<section id="outline-container-org7f16d57" class="outline-2">
<h2 id="org7f16d57">Zero sum games</h2>
<div class="outline-text-2" id="text-org7f16d57">
<p>
Agents have opposite utilities &#x2013; therefore can just use single value with plus and minus sign
obviously adversarial, only competitive behavior
</p>
</div>
</section>
<section id="outline-container-org99c01cb" class="outline-2">
<h2 id="org99c01cb">General games</h2>
<div class="outline-text-2" id="text-org99c01cb">
<p>
Independent utilities, Therefore diverse behaviors(cooperation, indifference, competition, &#x2013;)
</p>

<p>
Single-Agent trees
the Value of a state is the best achievable outcome from that state
minimax search is done for
deterministic, zero sum games
the search is done over state space search tree.
best achievable utility against a rational adversary
</p>


<p>
Adversarial search and trees
</p>

<p>
Minimax values
</p>

<p>
Pruning
</p>

<p>
Resource limits and evaluation functions
</p>
</div>
</section>
<section id="outline-container-orgd98eb26" class="outline-2">
<h2 id="orgd98eb26">Expectimax search</h2>
<div class="outline-text-2" id="text-orgd98eb26">
<p>
As evidenced by the name, it stems from, and bears similarity to minimax search, but it assumes that
The adversary is not rational(optimal), but instead nondeterministic with fixed probability.
</p>


<p>
*Monte Carlo Tree Search(MCTS)
MCTS is used when it is computationally infeasible to search game space. For example, for most games, using
alpha-beta search is accompanied by a fixed horizon, since search space is too large.
However, to be good at games, you need to search deep in the search tree.
</p>

<p>
The key ideas of MCTS is:
</p>
<ol class="org-ol">
<li>Evaluation by rollouts
play multiple games to termination from a state s, count wins and losses. Rollouts means that you just roll out
possible game states. W/L ratio is used to determine policy.</li>
</ol>
<p>
2? Selective search
  explore parts of the tree that will help improve the decision at the root, regardless of depth. Of course, you
  need to have correct heuristics, to determine where to search.
</p>



<p>
UCB Heuristics.    
UCB1 formula can determine how to combine promising and uncertain policy
</p>

<p>
Utility (Win rate ) + sqrt(logN(parent(n))/Ncurrent(n))
</p>


<p>
alpha-beta pruning
</p>
</div>
</section>
</article>
<footer id="postamble" class="status">
<p class="author">Author: user0</p>
<p class="date">Created: 2026-02-17 Tue 14:28</p>
<p class="validation"><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</footer>
</body>
</html>
